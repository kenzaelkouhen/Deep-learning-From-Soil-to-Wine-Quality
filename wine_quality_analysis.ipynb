{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "1. Merging the complete DF",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import glob\nimport pandas as pd\nimport numpy as np\nimport math\nfrom matplotlib import pyplot as plt\n\nall_dataframes =    [ ]\nall_dataframes.append(pd.read_csv('soil_analysis.csv'))\nall_dataframes.append(pd.read_csv('leaf_analysis.csv'))\nall_dataframes.append(pd.read_csv('petiole_analysis.csv'))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "2. Rename column's name",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for df in all_dataframes:\n        print(df.columns.values)\n\ni = 1\nfor df in all_dataframes:\n        m = {}\n        for name in df.columns.values[6:]:\n                m[name] = str(name+str(i))\n        i += 1\n        all_dataframes[i-2] = df.rename(columns=m)\n\nfor df in all_dataframes:\n        print(df.columns.values)\n\ndf = all_dataframes[0]\ndf = pd.merge(df, all_dataframes[1], how='outer', on=['ID Company', 'ID Lot', 'Nation', 'Province', 'Place', 'Year'])\ndf = pd.merge(df, all_dataframes[2], how='outer', on=['ID Company', 'ID Lot', 'Nation', 'Province', 'Place', 'Year'])\n\ndf.to_csv('complete_dataset.csv')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "3. Remove records having values of only 1 phase out of the three (soil/leaf/petiole)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "features = df.columns.values\n\nfeatures_soil = features[6:30]\nfeatures_leaf = features[30:41]\nfeatures_petiole = features[41:]\n\ndef check_empty(sample, features):\n        for feature in features:\n                if not math.isnan(sample[feature]):\n                        return 0\n        return 1\n\nto_drop = []\nfor index, row in df.iterrows():\n        if check_empty(row, features_soil) + check_empty(row, features_leaf) + check_empty(row, features_petiole) >= 2:\n                to_drop.append(index)\ndf = df.drop(to_drop)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "3.1 Integration height values",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ita_h = pd.read_csv(\"additional_datasets/italy_heights.csv\")\nslo_h = pd.read_csv(\"additional_datasets/slovenia_heights.csv\")\ncro_h = pd.read_csv(\"additional_datasets/croatia_heights.csv\")\nspa_h = pd.read_csv(\"additional_datasets/spagna_heights.csv\")\n\n\nita_h['HEIGHT'] = ita_h['HEIGHT'].str.replace(',', '.').astype(float)\n\nheights = []\nfor index, row in df.iterrows():\n        if row[\"Nation\"] == \"Italia\":\n                i = ita_h.loc[ita_h[\"NAME\"] == row[\"Place\"]].index.values[0]\n                heights.append(ita_h.at[i, \"HEIGHT\"])\n        elif row[\"Nation\"] == \"Slovenia\":\n                i = slo_h.loc[slo_h[\"NAME\"] == row[\"Place\"]].index.values[0]\n                heights.append(slo_h.at[i, \"HEIGHT\"])\n        elif row[\"Nation\"] == \"Croazia\":\n                i = cro_h.loc[cro_h[\"NAME\"] == row[\"Place\"]].index.values[0]\n                heights.append(cro_h.at[i, \"HEIGHT\"])\n        elif row[\"Nation\"] == \"Spagna\":\n                i = spa_h.loc[spa_h[\"NAME\"] == row[\"Place\"]].index.values[0]\n                heights.append(spa_h.at[i, \"HEIGHT\"])\n        else:\n                print(\"Error: Nation not allowed\")\n\n\nif \"Height\" not in df.columns.values.tolist():\n        df.insert(6, \"Height\", heights, True)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "3.2 Integration temperature and humidity",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "temp_hum = pd.read_csv(\"temperature&humidity/complete_temp_hum.csv\")\ntemp_hum = temp_hum.drop(temp_hum.columns.values[0], axis = 1)\n\nphase2 = temp_hum.loc[temp_hum[\"Phase\"] == 2].copy()\nphase2.rename(columns = {\"T avg [˚C]\": \"T avg 2\", \"T min [˚C]\": \"T min 2\", \"T max [˚C]\": \"T max 2\", \"Humidity\": \"Humidity2\"}, inplace = True)\nphase2 = phase2.drop(\"Phase\", axis = 1)\n\nphase3 = temp_hum.loc[temp_hum[\"Phase\"] == 3].copy()\nphase3.rename(columns = {\"T avg [˚C]\": \"T avg 3\", \"T min [˚C]\": \"T min 3\", \"T max [˚C]\": \"T max 3\", \"Humidity\": \"Humidity3\"}, inplace = True)\nphase3 = phase3.drop(\"Phase\", axis = 1)\n\ntemp_hum = pd.merge(phase2, phase3, how = 'outer', on = [\"Province\", \"Year\"])\n\ndf = pd.merge(df, temp_hum, how = \"left\", on = [\"Province\", \"Year\"])\ndf.to_csv('complete_dataset.csv')\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "3.3 Integration coordinates",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "coo = pd.read_csv(\"geo_coords.csv\", sep = \"\\t\")\ncoo = coo.rename({\"City\" : \"Place\"}, axis = 1)\ndf = pd.merge(df, coo, how=\"left\", on=\"Place\")\n\ndf.to_csv('complete_dataset.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "4. Data cleaning: fix the '<x' and '>x' data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "feature = \"Cu3\"\nless = []            #elements presenting '< x'\ngreat = []          #elements presenting '> x'\nnormal = []        #legit elements (Nan are not considered here)\n\nfor val in df[feature]:\n        if str(val)[0] == '<':\n                temp = str(val)[1:]\n                temp = float(temp.replace(',', '.'))\n                if temp not in less:\n                        less.append(temp)\n                        print(\"less than \"+str(less[-1]))\n        elif str(val)[0] == '>':\n                temp = str(val)[1:]\n                temp = float(temp.replace(',', '.'))\n                if temp not in great:\n                        great.append(temp)\n                        print(\"greater than \"+str(great[-1]))\n        elif not math.isnan(float(str(val).replace(',', '.'))):\n                temp = float(str(val).replace(',', '.'))\n                normal.append(temp)\n\nplt.plot(normal)\nprint(\"MIN = \"+str(min(normal)))\nprint(\"MAX = \"+str(max(normal)))\n\nfeatures = df.columns.values[7:]\ntypes = df.dtypes[7:]\n\nprint(\"Max values of the affected columns\")\n\ni = 0\nfor feature in features:\n        #only columns with dtype=object need to be fixed\n        if types[i] != \"object\":\n                i += 1\n                continue\n        new_col = []\n        \n        #find the max value\n        m = 0\n        for val in df[feature]:\n                if str(val)[0] == '<':\n                        continue\n                elif str(val)[0] == '>':\n                        temp = temp = str(val)[1:]\n                        temp = float(temp.replace(',', '.'))\n                        m = max(m, temp)\n                else:\n                        temp = float(str(val).replace(',', '.'))\n                        m = max(m, temp)\n        print(feature+\"        \"+str(m))\n        \n        #compose the new/clean column of values\n        for val in df[feature]:\n                temp = val\n                if str(val)[0] == '<':\n                        temp = 0.0\n                elif str(val)[0] == '>':\n                        temp = m\n                else:\n                        temp = float(str(val).replace(',', '.'))\n                new_col.append(temp)\n        \n        #replace old column with the clean one\n        df[feature] = new_col\n        i += 1\n\ndf.to_csv('complete_dataset.csv')\n\n\n#Handling sparse missing values\n#Let's count first the number of missing values in the current version of the dataset\n\nfeatures = df.columns.values\nfeatures_soil = features[7:31]\nfeatures_leaf = features[31:42]\nfeatures_petiole = features[42:53]\n\n\ndef count_missing(record, features, miss):\n        c = 0\n        for feature in features:\n                if math.isnan(record[feature]):\n                        miss[feature] += 1\n                        c += 1\n        return c\n\nmiss = {}\nfor feature in features[7:53]:\n        miss[feature] = 0\n\nfor index, row in df.iterrows():\n        if not check_empty(row, features_soil):\n                count_missing(row, features_soil, miss)\n        if not check_empty(row, features_leaf):\n                count_missing(row, features_leaf, miss)\n        if not check_empty(row, features_petiole):\n                count_missing(row, features_petiole, miss)\n\nprint(\"Total number of rows = \" + str(df.shape[0]))\n\nplt.figure(figsize=(20,6))\nplt.bar(miss.keys(), list(miss.values()), align='center')\nplt.xticks(rotation=90)\n#plt.xticks(range(len(miss)), list(miss.keys()))\nplt.show\n\np1 = df.filter(items = features_soil, axis = 1)\nfor index, row in p1.iterrows():\n        if check_empty(row, p1.columns.values):\n                p1 = p1.drop(index)\n\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=4)\np1 = imputer.fit_transform(p1)\np1 = pd.DataFrame.from_records(p1, columns = features_soil)\n\n\np1_original = df.filter(items = features_soil, axis = 1)\np1_final = []\n\ni = 0\nfor index, row in p1_original.iterrows():\n        if check_empty(row, p1_original.columns.values):\n                p1_final.append(row)\n        else:\n                p1_final.append(p1.iloc[i].copy())\n                i += 1\np1_final = pd.DataFrame.from_records(p1_final)\n\nfor c in p1_final.columns:\n        df[c] = p1_final[c]\ndef count_missing(record, features, miss):\n        c = 0\n        for feature in features:\n                if math.isnan(record[feature]):\n                        miss[feature] += 1\n                        c += 1\n        return c\n\nmiss = {}\nfor feature in features[7:53]:\n        miss[feature] = 0\n\nfor index, row in df.iterrows():\n        if not check_empty(row, features_soil):\n                count_missing(row, features_soil, miss)\n        if not check_empty(row, features_leaf):\n                count_missing(row, features_leaf, miss)\n        if not check_empty(row, features_petiole):\n                count_missing(row, features_petiole, miss)\n\nprint(\"Total number of rows = \" + str(df.shape[0]))\n\nplt.figure(figsize=(20,6))\nplt.bar(miss.keys(), list(miss.values()), align='center')\nplt.xticks(rotation=90)\n#plt.xticks(range(len(miss)), list(miss.keys()))\nplt.show\n\ndf.to_csv('complete_dataset.csv')\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}